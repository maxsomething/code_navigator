# --- AI Configuration ---
# The URL where Ollama is running (default is local)
OLLAMA_HOST=http://127.0.0.1:11434

# Primary Chat Model (Recommended: codellama, mistral, or llama3)
# This model answers your questions about the code.
OLLAMA_MODEL_CHAT=llama3.1:8b

# Summarizer Model
# Used to generate the documentation summaries for files.
OLLAMA_MODEL_SUMMARIZER=llama3.1:8b

# Embedding Model (HuggingFace)
# Used to convert code definitions into vectors for RAG (Retrieval Augmented Generation).
# 'all-MiniLM-L6-v2' is fast and effective for running locally on CPU.
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2

# --- App Settings ---
# Directory to store generated graph HTML files and documentation
OUTPUT_DIR=data/output
LOG_LEVEL=INFO

# --- Hardware Acceleration ---
# Set to the number of layers to offload to GPU (if available).
# Set to 0 for CPU-only.
OLLAMA_NUM_GPU=30
# Context window size (bytes). 4096 is usually sufficient for code context.
OLLAMA_NUM_CTX=4096
